# 归一化

### 1. 归一化为什么能提高梯度下降法求解最优解的速度？



![img](https://img-blog.csdnimg.cn/20210217135818782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTMzMjAwOQ==,size_16,color_FFFFFF,t_70)



如图所示，蓝色的圈圈代表的是两个特征的等高线。其中左图两个特征$X_1$和$X_2$ 的区间相差非常大，$X_1$区间是[0,2000], $X_2$区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；
而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。

因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。例如，我们在用Unet做天气预报的模式融合时，如果不把训练目标两米温度的值进行归一化，直接用原来[200, 300]开尔文的温度进行训练，loss就会不收敛，最后甚至达到inf。

### 2. 什么时候归一化有用

一些分类器需要计算样本之间的**距离**（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要**取决于这个特征**，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。

### 3 归一化的类型

**1）线性归一化**
​                                                                              $x' = \frac{x - \text{min}(x)}{\text{max}(x)-\text{min}(x)}$

 这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。

**2）标准差标准化**

经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：
​                                                                          $x^* = \frac{x-\mu}{\sigma}$
其中μ为所有样本数据的均值，σ为所有样本数据的标准差。

**3）非线性归一化**
经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。



### 4. 面试问题

**Q: 哪些机器学习算法不需要做归一化处理？**
A: 树模型不需要归一化。更广义地说，概率模型不需要归一化。因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。而像svm、KNN、KMeans之类的最优化问题就需要归一化。

树模型不需要归一化的原因是：

数值缩放，不影响分裂点位置。因为当树的一个节点分裂时，都是按照特征的值进行排序的。如果排序的顺序不变，那么分裂点就不会有不同。但是对于线性模型，比如说LR，有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。
另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的。

**Q: SVM和LR需要不需要归一化？**
A: 有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。有些模型伸缩有与原来等价，比如LR。但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最好进行数据归一化。
补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。