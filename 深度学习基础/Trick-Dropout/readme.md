# Dropout

![img](https://miro.medium.com/max/2842/1*TfTdymI0ot6ZOdwVUK1V2w.png)

Dropout是一种典型的**【防止过拟合】**的方法。

过拟合的原因是算法的学习能力过强；而训练样本过少不能对整个空间进行分布估计。




### 1. 分布式特征表达
分布式表征（Distributed Representation），简单来说，就是当我们表达一个概念时，神经元和概念之间不是**一一映射**存储的，它们之间的关系是多对多。具体而言，就是**一个概念可以用多个神经元共同定义表达，同时一个神经元也可以参与多个不同概念的表达**。

分布式表征表示有很多优点，其中一点是当部分神经元发生故障时，信息的表达不会出现覆灭性的破坏。



### 2. Dropout工作原理

Dropout，也称为“随机失活”，是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。

**训练阶段：**

以概率p主动临时性地忽略掉部分隐藏节点，算法步骤如下：

1. 首先随机删掉网络中的一些*隐藏*神经元，一般情况下输入层、输出层的神经元保持不变

2. 把输入通过修改后的网络前向传播，删除的神经元不进行前向传播，即传给下一层的值是0；然后把得到的损失结果通过修改后的网络反向传播。一个batch的训练样本执行完这个过程后就按照随机梯度下降法更新没有被删除的神经元对应的参数（w，b）。这样，每一个batch训练之后，**只有那些没有被删除的神经元更新了参数**，那些被删除的神经元未更新参数。

3. 恢复被删掉的神经元。此时刚才被删除的神经元恢复原样，而没有被删除的神经元已经有所更新。

4. 不断重复上述过程1，2，3.

**测试阶段：**

预测的时候，每一个单元的参数要乘以p。![img](https://img-blog.csdnimg.cn/img_convert/076049b940077f7aff89a3d6d2852043.png)





### 1.3 为什么Dropout防止过拟合

**（1）数据层面**

对于每一个dropout后的网络，进行训练时，相当于做了Data Augmentation。比如，对于某一层，dropout一些单元后，形成的结果是(1.5，0，2.5，0，1，2，0)，其中0是被drop的单元。这样每一次dropout其实都相当于增加了样本。

**（2）模型层面**

**i. Dropout思想类似于集成学习中的Bagging思想 (Bootstrap)**

由学习阶段可知，每一次训练都会按keep_probability=p来保留每个神经元，这意味着每次迭代过程中，随机删除一些神经元，这就意味着在多个"残缺"的神经网络中，每次都进行随机的特征选择，这要比仅在单个健全网络上进行特征学习，其泛化能力来得更加健壮。

Dropout 能够达到Bagging中**取平均的作用：** 先回到正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。（例如 3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果）。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。每次训练随机dropout掉不同的隐藏神经元，网络结构已经不同，这就类似在训练不同的网络，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。

**ii. 减少神经元之间共适应关系：** 因为dropout导致两个神经元不一定每次都在一个网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况， 迫使网络去学习更加鲁棒的特征。换句话说，假如神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。

### 1.4 Dropout 与 Bagging 有何不同？

- 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型**共享参数**，其中每个模型继承父神经网络参数的不同子集。
- 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。



## 2. 其他解决过拟合的方法

- a.早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练
- b.数据集扩增：原有数据增加；原有数据加随机噪声，经过变换生成新的数据（数据增强）
- c.交叉验证：在验证集上准确率开始下降时，停止训练
- d.特征选择/特征降维
- e. L1/L2或者其他正则化方法









