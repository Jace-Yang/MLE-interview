# 炼丹经验 - 超参数调优

往往网络的超参数多达几十个，要是都精调的话，那岂不是得 gg， 因此往往是对重要参数**精调**，对次要参数**粗调**。

## 1. 常见的超参数

CNN 网络参数

| 超参数      | 说明          | 推荐值                                         |
| ----------- | ------------- | ---------------------------------------------- |
| kernel size | 卷积核的 size | 一般为奇数：[7 * 7], [5 * 5], [3 * 3], [1 * 1] |
| kernel num  | 卷积核的数量  | 一般在 [100, 600] 间探索                       |

优化参数

| 超参数        | 说明                           | 推荐值     |
| ------------- | ------------------------------ | ---------- |
| learning rate | 最重要的参数，需要精调         | 下文有推荐 |
| batch size    | 次要重要参数，需要精调         | [1: 1024]  |
| dropout       | 解决过拟合的重要参数，需要精调 | [0: 0.5]   |

正则化参数：

| 超参                          | 说明 | 推荐值    |
| ----------------------------- | ---- | --------- |
| L2 权重衰减系数(weight decay) |      | [0, 1e-4] |

（L2正则化的目的就是为了让权重衰减到更小的值，在一定程度上减少模型过拟合的问题，所以**权重衰减**也叫L2正则化，系数$λ$就是权重衰减系数。）



## 2. 几个重要的超参数

### 1. 学习率 -- 最重要的超参数

学习率直接控制着梯度更新时的量级，从而直接影响模型的优化与最终的有效容量。 幸运的是，对于学习率的设置，已经有一套行之可效的指导方案了， 针对不同的优化器，有不同的设置区间。

如果是微调，那么学习率要降低两个数量级左右（参考 Bert 的 adam 学习率)

| 优化器   | 设置范围     |
| -------- | ------------ |
| SGD      | [1e-2 ,1e-1] |
| Momentum | [1e-3, 1e-2] |
| Adagrad  | [1e-3, 1e-2] |
| Adadelta | [1e-2, 1e-1] |
| RMSprop  | [1e-3, 1e-2] |
| Adam     | [1e-3, 1e-2] |
| Nadam    | [1e-3, 1e-2] |

### 2. batch size

一般情况下， batch size 往往以 128 为起点上下调整。注意，batch size 要设置为 2 的幂次方， 范围在 [1, 1024] 之间。

此外，需要一提的是 Batch Normalization 与 batch size 息息相关，如果你使用了 Batch Normalization， 那么 batch size 就不能设的太小， 这点在Batch Normalization 那一节中有详细解释。

##### 合理增加 batch size 有何好处？

- 并行化效率提高，对于相同数据量的处理速度进一步加快。
- 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

##### 盲目增大 Batch_Size 有何坏处？

- 内存，显存容量可能撑不住
- 跑完一次 epoch 所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
- Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

### 3. dropout

dropout 往往设置先为 0.5， 然后在 [0.0, 0.5] 范围内精调。

Dropout 往往会在卷积层和全连接层之间是有来防止过拟合。 使用 Dropout 需要注意两点：

- 在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；
- 不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；

### 4. 优化器参数

对于优化器的一些参数，往往采取默认值即可，这是因为，默认值都是论文最初的设置，一般都能够获得不错的表现。



## 超参数调优策略

### 1. 网格搜索

- 定义一个 n 维的网格，每一格都有一个超参数。
- 对于每个维度，定义可能的取值范围
- 搜索所有可能的配置并获得最佳结果

**缺点：**该方法痛点真的很痛，那就是：**维数灾难**。 随着要精调的超参数的增加，搜索在时间复杂度上也会增加的越多（指数级别），最终使得该策略不可行。

**优点：** 如果采用较大的搜索范围以及较小步长，该方法有很大概率能找到全局最优值

因此， 一般尽可能少的去调节次要超参数，比如优化算法默认 Adam 等。此外， 先进行粗调来寻找全局最优值可能的位置，然后采用精调的策略寻找更精确的最优值。

一般只有超参数在 4 个以内才使用网格搜索，不然太费时间了。

### 2. 随机搜索

随机搜索在搜索范围内随机选取样本点，它认为如果样本点集足够大，那么通过随机采样也能大概率的找到全局最优值或其近似值。

- 优点： 比网格搜索要快
- 缺点：结果无法保证，很依靠调参经验。

一般都是以推荐超参数设置方案来作为第一次的设置，然后围绕这个设置点**上下浮动**。

![1639278634907](C:\Users\zh-wa\AppData\Roaming\Typora\typora-user-images\1639278634907.png)



