# GBDT, Xgboost, Lightgbm 细节问题

关于Xgboost：https://github.com/hannawong/MLE-interview/tree/master/2.%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20-%20XGBoost

### 1. GBDT

#### 1.1 RF和GBDT的区别

**相同点：**都是由多棵树组成，最终的结果都是由多棵树一起决定。

**不同点：**

- **集成学习**：RF属于bagging思想，而GBDT是boosting思想。因此，RF不断的降低模型的方差，而GBDT不断的降低模型的偏差；RF不易过拟合，GBDT容易过拟合；RF的树可以并行生成，而GBDT只能串行。
- **训练样本**：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
- **最终结果**：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合（前向分步算法）
- **数据敏感性**：RF对异常值不敏感，而GBDT对异常值比较敏感



#### 1.2 比较LR和GBDT，什么情景下GBDT不如LR

先说说LR和GBDT的区别：

- LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
- GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；

当在**高维稀疏特征**的场景下，LR的效果一般会比GBDT好。先看一个例子：

假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只有10个正样本，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的**过拟合**。

那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：y = W1*f1 + Wi*fi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？

仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚。但是树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。

这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：**带正则化的线性模型比较不容易对稀疏特征过拟合。**



### 2. Xgboost

#### 2.1 Xgboost加入的一些新特性和作用

1. 在损失函数中加入了树的复杂度（叶子节点个数+叶子节点权重）作为正则项。作用：防止过拟合
2. 在前向分步算法中，使用了二阶Taylor展开。作用：精准性+可扩展性（后文介绍）
3. 对每个特征 k 都确定 l 个候选切分点进行分割。作用：加速
4. 系数衰减(shrinkage): 学习率/步长，为了给后面的训练留出更多的学习空间。作用：防止过拟合
5. 列抽样：训练的时候只用一部分特征进行分裂，类似随机森林的方法。作用：防止过拟合
6. 解决了数据稀疏的问题
7. 分块并行：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用；并且支持并行查找每个特征的分割点。作用：加速
8. CPU cache 命中优化： 使用缓存预取的方法，对每个线程分配一个连续的缓冲区，读取每个block中样本的梯度信息并存入连续的缓冲区中。作用：加速

9. Block处理优化：使用一个独立的线程专门从硬盘读取数据，加载到内存中；块压缩，读取的时候用另外的线程解压。块分区，将特征block分区存放在不同的硬盘上，以此来增加硬盘IO的吞吐量。作用：加速



#### 2.2 XGBoost与GBDT的区别

[参考链接](https://www.zhihu.com/question/41354392)

首先，XGboost是GBDT的工程优化，其本质思想没有区别。

- **基分类器**：传统GBDT以CART树作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的logistic regression（分类问题）或者线性回归（回归问题）。
- **导数**：传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开。xgboost支持自定义代价函数，只要函数可一阶和二阶求导。
- **正则项/列抽样/系数衰减的防止过拟合方法**
- **并行**：xgboost的并行不是树粒度的并行，而是在特征粒度上的。决策树的学习最耗时步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中**重复**地使用这个结构减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- **可并行的近似直方图算法**。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
- 其余见2.1

#### 2.3 XGBoost为什么使用泰勒二阶展开

- **精准性**：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。就像牛顿法对梯度下降的方向有更精确的把握。
- **可扩展性**：损失函数支持自定义，只需要新的损失函数一阶二阶可导即可。



#### 2.4 XGBoost如何处理缺失值

XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：

- 在特征k上寻找最佳划分点时，不会对该列特征缺失的样本进行遍历，而只对该列特征值为**不缺失的**样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找划分点的时间开销。
- 在逻辑实现上，为了保证完备性，会将该特征值缺失的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。
- 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。

![img](https://static.careerengine.us/api/aov2/https%3A_%7C__%7C_mmbiz.qpic.cn_%7C_mmbiz_png_%7C_90dLE6ibsg0fkqnx5yOhtlvx8dFgk1DvVfp2pmTsZ0yX0A2usH3afam4cJb7lQNIJGb3N2VZicclrfoRqM6MHhtQ_%7C_640%3Fwx_fmt%3Dpng)

#### 2.5 XGBoost中的一棵树的停止生长条件

- 当新引入的一次分裂所带来的增益**Gain<0**时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。
- 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数**max_depth**。
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个**叶子的样本权重**低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。

#### 2.6 XGBoost如何处理不平衡数据

对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost中有自带的**scale_pos_weight**来平衡正样本和负样本的权重。scale_pos_weight可以设置为数据中负样本数量/正样本数量。设置scale_pos_weight、 给minority样本设置大的权重、把**损失函数**中minority错分率设为大权重、给minority重采样至平衡 这四种方法是等价的。具体实验见：https://blog.csdn.net/qq_32103261/article/details/106664227



#### 2.7 XGBoost的可扩展性（Scalable）性如何体现

- **基分类器的scalability**：弱分类器可以支持CART决策树，也可以支持LR和Linear。
- **目标函数的scalability**：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。
- **学习方法的scalability**：Block结构支持并行化，支持核外计算。



#### **2.8 XGBoost如何评价特征的重要性**

我们采用三种方法来评判XGBoost模型中特征的重要程度：

- **weight** ：该特征在所有树中**被用作分割样本的特征**的总次数。

- **gain** ：该特征在其出现过的所有树中产生的**平均增益**。

- **cover** ：该特征在其出现过的所有树中的平均覆盖范围。


#### 2.9 XGBooost常见超参数

- learning rate
- n_estimator

- max_depth，每棵子树的最大深度。

- min_child_weight，子节点权重阈值。如果一个节点分裂以后，其所有**子节点权重之和**都大于该阈值，该叶子节点才能划分。

  - 这里的“节点权重”计算公式为：

    ![img](https://pic1.zhimg.com/80/v2-5885b61a9341b22b53d05753a89da797_1440w.png)

- gamma, 也称作最小划分损失min_split_loss。指的是对于一个叶子节点，当对它采取划分之后，**损失函数的降低值**的阈值。

  - 如果大于该阈值，则该叶子节点值得继续划分
  - 如果小于该阈值，则该叶子节点不值得继续划分

- subsample：对训练的采样比例

- colsample_bytree：对特征的采样比例

- alpha 是L1正则化系数，lambda 是L2正则化系数



#### **2.10 XGBoost模型如果过拟合了怎么解决**

当出现过拟合时，有两类参数可以缓解：

第一类参数：用于直接控制模型的复杂度。包括max_depth, min_child_weight, gamma 等参数

第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample,colsample_bytree

还有就是直接减小learning rate，但需要同时增加estimator 参数。